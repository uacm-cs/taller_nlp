{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"./data/data_emotions_es.json\", lines=True)\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "dataset.groupby(['klass']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparar los conjuntos de datos  (datasets) para entrenamiento y para probar el rendimiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creación de la matriz de pesos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords \n",
    "    return [t for t in texto.split() if t not in _STOPWORDS]\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "print(\"tamaño de la matriz: \", X_tfidf.shape)\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convierte los ejemplos de prueba al mismo espacio de representación de datos del conjunto de entrenamiento que aprendió el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entrenar al clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Support Vector Machine (SVM)\n",
    "\n",
    "- ### El **Método de Máquinas de Vectores de Soporte (SVM, por sus siglas en inglés)** \n",
    "    - #### Es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión. \n",
    "    - #### Su objetivo principal es encontrar un hiperplano en un espacio de características que maximice el margen entre diferentes clases de datos.\n",
    "\n",
    "- ### Funcionamiento:\n",
    "\n",
    "    1. **Clasificación**: \n",
    "        - SVM busca el hiperplano que mejor separa las diferentes clases de datos con el mayor margen posible. \n",
    "        - Los puntos de datos que están más cerca del hiperplano se llaman vectores de soporte, ya que definen el margen de separación.\n",
    "\n",
    "    2. **Regresión**: \n",
    "        - En el caso de regresión, SVM intenta encontrar un hiperplano que se ajuste a los datos de manera que la desviación entre las predicciones y los valores reales sea mínima.\n",
    "\n",
    "- ### Características Clave:\n",
    "\n",
    "    - **Hiperplano Óptimo**: Maximiza la distancia entre el hiperplano y los vectores de soporte. En la fig. H3 maximiza el margen\n",
    "    - **Espacio de Características**: Puede utilizar el truco del núcleo (kernel trick) para transformar los datos en un espacio de dimensiones más altas, facilitando la separación no lineal.\n",
    "    - **Regularización**: Controla el equilibrio entre el margen máximo y la clasificación correcta de los datos de entrenamiento.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/fig-svm.png\" width=\"300\" style=\"background-color:white;\">\n",
    "</center>\n",
    "\n",
    "###### Fuente: https://es.wikipedia.org/wiki/Máquinas_de_vectores_de_soporte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos LinearSVC del paquete sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clasificador_svc = LinearSVC(random_state=42)\n",
    "\n",
    "# Entrenamos al clasificador\n",
    "\n",
    "clasificador_svc.fit(X_tfidf, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción de los datos del conjuntos de prueba con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clasificador_svc.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección de los resultados de los primeros N ejemplos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"textos: \", X_test[:5])\n",
    "print(\"clase esperada: \",Y_test[:5])\n",
    "print(\"clase predicha: \", y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar la predicción de la clase original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obten las primeras N predicciones\n",
    "pred =  y_pred[:5] \n",
    "pred_ori = le.inverse_transform(pred)\n",
    "pred, pred_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluando el desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación\n",
    " - #### Las métricas precisión, recall y F1 son fundamentales para evaluar el rendimiento de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_precision-recall.png\" width=\"300\">\n",
    "\n",
    "##### Fuente: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_matriz-confusion.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP=True Positive\n",
    "\n",
    "TN=True Negative\n",
    "\n",
    "FP=False Positive (Error tipo I: ejemplo, se considera que el paciente está enfermo, pero en realidad está sano)\n",
    "\n",
    "FN=False Negative ( Error tipo II: ejemplo, se considera que el paciente está sano, pero en realidad está enfermo)\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{total~ TP + total~TN}{total~muestras} $$\n",
    "\n",
    "$$ Precision_c = \\frac{ TP_c}{TP_c + FP_c} $$\n",
    "\n",
    "$$ Recall_c = \\frac{ TP_c}{TP_c + FN_c} $$\n",
    "\n",
    "$$ F1-score_c= 2 \\times \\frac{ Precision_c \\times Recall_c}{Precision_c + Recall_c} $$\n",
    "\n",
    "$$ macro-F1-score= \\frac{ 1 }{|Clases|} \\sum{F1-score_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la clase 0, la precisión es la siguiente\n",
    "tp= 82\n",
    "fp = 31+11+33\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print(\"P=\", precision_score(Y_test, y_pred, average='macro'))\n",
    "print(\"R=\", recall_score(Y_test, y_pred, average='macro'))\n",
    "print(\"F1=\", f1_score(Y_test, y_pred, average='macro'))\n",
    "print(\"Acc=\", accuracy_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección del desempeño por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred, digits=4, zero_division='warn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guadar el modelo para despliegue de la aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se recomienda generar un módulo para el despliegue del modelo (Por ejemplo clasificadorTODO.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from clasificadorTODO import ClasificadorTODO\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_json(\"./data/data_emotions_es.json\", lines=True)\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "clasificador_svc = ClasificadorTODO()\n",
    "clasificador_svc.setLabelEncoder(le)\n",
    "clasificador_svc.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# guardar el modelo de clasificación\n",
    "with open('./modelo_svc.pkl','wb') as file:\n",
    "    pickle.dump(clasificador_svc, file)\n",
    "    \n",
    "print(clasificador_svc.predict([X_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from clasificador import Clasificador\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_json(\"./data/data_emotions_es.json\", lines=True)\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "clasificador_svc = Clasificador()\n",
    "clasificador_svc.setLabelEncoder(le)\n",
    "clasificador_svc.fit(X_train, Y_train)\n",
    "\n",
    "# guardar el modelo de clasificación\n",
    "with open('./modelo_svc.pkl','wb') as file:\n",
    "    pickle.dump(clasificador_svc, file)\n",
    "    \n",
    "#Probando que el clasificador funcione correctamente\n",
    "\n",
    "print(clasificador_svc.predict([X_test[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
